---
title: "M12 - Spatial Regression"
instructor: ""
course: "EPBI 5003"
---

# Module outline

1.  Data

2.  Spatial weights matrices

3.  Spatial autocorrelation

4.  Spatial regression

5.  In-class exercise

# 1. Data

We'll be using a commonly reference data set for exploring and learning spatial regression. The lip cancer case data among Males in Scotland counties from 1975 to 1980. We will use this data to assess patterns in the standardized incidence ratios (SIR), for cancer clustering, and estimate the risk of lip cancer relative to the proportion of the population in agriculture, fishing, or forestry (AFF), which is a risk factor for lip cancer given the increased exposure to sun light.

```{r}
# load libraries
library(tidyverse)
library(sf)

# the following code reads in the zip file from github with the scottish lip cancer shapefile
t1 <- tempfile()
t2 <- tempfile()
lmi <- download.file("https://raw.githubusercontent.com/Rsnead91/EPBI_5003/main/Modules/module12/lmi.zip",t1)
unzip(zipfile = t1, exdir = t2)
lmi <- st_read(t2)

# review the data types
str(lmi)

# review the raw data
head(lmi)
```

The variables included in the data are:

|           |                                              |
|:----------|:---------------------------------------------|
| CODENO    | Code converted to numeric (drop w prefix)    |
| AREA      | District polygon area                        |
| PERIMETER | District polygon perimeter                   |
| RECORD_ID | Unique ID                                    |
| DISTRICT  | District number 1-56                         |
| NAME      | Name of districts from Cressie (1993)        |
| CODE      | District code from WinBugs                   |
| CANCER    | Lip cancer cases from Cressie (1993)         |
| POP       | Population years at risk from Cressie (1993) |
| CEXP      | Expected cases from Lawson et al. (1999)     |
| AFF       | Outdoor industry from Lawson et al. (1999)   |

First we'll run a ggplot to make sure the projection looks right. Then, we will perform a simple exploratory analysis/quality check by calculating descriptive statistics and mapping the population, observed, expected, and % AFF.

```{r}
# take a look at the study area
# the crs code is set to ESPG 27700, the projection for OSGB 1936/British National Grid, which is commonly used for the UK
# you could find the crs by simply running: st_crs(lmi)
ggplot() +
  geom_sf(data = lmi)

# using the summary function to return descriptive statistics on the subset numeric data
summary(lmi[,c("POP","CANCER","CEXP","AFF")])

# mapping variables of interest

# population
ggplot() +
  geom_sf(data = lmi, aes(fill = POP))

# observed cancer cases
ggplot() +
  geom_sf(data = lmi, aes(fill = CANCER))

# expected cancer cases
ggplot() +
  geom_sf(data = lmi, aes(fill = CEXP))

# proportion of county in an outdoor industry
ggplot() +
  geom_sf(data = lmi, aes(fill = AFF))

```

After reviewing the descriptives, what patterns are you starting to notice?

Now let's calculate the SIR and map the results.

```{r}
# create a new variable for SIR by dividing the observed by expected # of cases
lmi$SIR <- lmi$CANCER / lmi$CEXP

# review new SIR variable
print(lmi[,c("CANCER","CEXP","SIR")])

# map SIRs
ggplot() +
  geom_sf(data = lmi, aes(fill = SIR)) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 1) +
  theme_void()


```

What are your thoughts about the patterns of the SIR for lip cancer? How do they differ or affirm your thoughts from reviewing the previous descriptives?

# 2. Spatial weights matrices

In order to start performing tests and running regressions, we need to set up our spatial weights matrix, or neighborhood matrix. The objects and functions used for identifying neighbors and creating weights are mainly from the [**spdep**](https://r-spatial.github.io/spdep/index.html) package. Depending on the spatial analysis and packages you are using, you may need your neighbor files or weights matrix in various object types, such as nb, list, or matrix. The first step to creating a spatial weight's matrix is to identify neighboring polygons.

## 2.1 Neighbors

### 2.1.1 Contiguity

To create a create a queen or rook contiguity neighbors file we can use `poly2nb()`, which takes our shapefile of polygons and identifies bordering neighbors.

```{r}
#install.packages("spdep)
library(spdep)

# create nb object for scotland county polygons
# 'queen = TRUE' tells R we want to use queen rather than rook contiguity. For rook, use 'queen = FALSE'
#?poly2nb
scot_nb <- poly2nb(lmi, queen = TRUE)

# reviewing assigned neighbors
plot(st_geometry(lmi), border = "grey")
plot(scot_nb, st_geometry(lmi), add = TRUE)

```

Each line connecting from one circle to another represents a neighbor connection. Notice the islands are not connected and the northern counties don't have many neighbors. How do you think that might effect regression estimates?

The neighbors file we just created was a first order contiguity. To create a second order contiguity neighbors file, we'll use `nblag()` to create a first and second order list of neighbors. We can then combine those two files with `nblag_cumul()`.

```{r}
# create a first and second order neighbors file by setting 'maxlag' to 2
#?nblag
nb_b_1_2 <- nblag(scot_nb, maxlag = 2) 

# combining first and second order neighbors
#?nblag_cumul
nb_b_1_2c <- nblag_cumul(nb_b_1_2)

# first order contiguity neighbors only
plot(st_geometry(lmi), border = "grey")
plot(scot_nb, st_geometry(lmi), add = TRUE)

# first and second order contiguity neighbors
plot(st_geometry(lmi), border = "grey")
plot(nb_b_1_2c, st_geometry(lmi), add = TRUE)
```

### 2.1.2 Distance-based

Contiguity neighbors are simple to create and understand but always best for regression analyses. Alternatively, we can use distance-based methods to identify neighbors, such as k-nearest-neighbors (knn).

**knn based on the number of nearest neighbors:**

```{r}
# use centroids rather than polygons for knn
centroids <- st_centroid(lmi)

# create neighbor file for nearest 5 neighbors
#?knearneigh
#?knn2nb
nb_5 <- knn2nb(knearneigh(centroids, k  = 5))

# reviewing knn = 5 neighbors
plot(st_geometry(lmi), border = "grey")
plot(nb_5, st_geometry(lmi), add = TRUE)

```

Now the islands are connected to mainland Scotland and some neighbors from the north are connected to counties relatively far away. How might you modify the weights specification?

NOTE: If you use knn, you need to link the data from the centroids to the original polygons and also force symmetry in the weights matrix, `knn2nb(sym = TRUE)`, for the regression to run.

**knn based on distance to neighbors:**

```{r}
# assigning neighbors based on distance between county centroids
# d1 = lower bound for distance in km, d2 = upper bound for distance in km

# finding the maximum distance that any centroid is from another to ensure all counties have at least 1 neighbor
#?nbdists
#?unlist
#?max
max_dist <- max(unlist(nbdists(knn2nb(knearneigh(centroids)), centroids)))
max_dist

#?dnearneigh
nb_d <- dnearneigh(centroids, d1 = 0, d2 = max_dist)

# reviewing knn neighbors up to 10km away
plot(st_geometry(lmi), border = "grey")
plot(nb_d, st_geometry(lmi), add = TRUE)

```

How do you compare the results from the contiguity and knn neighbors? Which do you think is more appropriate for our data?

## 2.2 Weights

The second step in creating a weights matrix is to assign weights to neighbors and convert the `nb` object to a list or matrix.

**Binary:**

In it's most basic form, a binary weights matrix assigns neighbors a weight of 1 and non-neighbors a weight of 0. Additional weighting "styles", or schemes, are row standardized, globally standardized, or the globally standardized / the number of neighbors.

```{r}
# creating spatial weights by assigning basic binary coding based on the contiguity neighbors file
# style = "B" -> 1 = neighhbor, 0 = non-neighbor
# spatial weights in a 'list'
# 'zero.policy = TRUE' allows for 'islands' which are polygons with no neighbors
#?nb2listw
nbwt_b_lst <- nb2listw(scot_nb, style = "B", zero.policy = TRUE)

# elements in the list
# coding style
nbwt_b_lst$style

# shows the neighbors, by row number, for the first polygon
nbwt_b_lst$neighbours[1]

# shows the weights for the first polygon
nbwt_b_lst$weights[1]
  
# spatial weights in a matrix
#?listw2mat
nbwt_b_mat <- listw2mat(nbwt_b_lst)

# weight matrix among the first ten polygons
nbwt_b_mat[1:10, 1:10]

```

**Inverse Distance Weighting:**

Inverse distance weighting performs better for spatial regression analyses than binary weighting schemes. From our contiguity neighbors file, we can use `nbdists()` to calculate the inverse distance weights. We will use the first and second order neighbors to calculate the final weights matrix.

```{r}
# calculating the distance from polygon centroid to neighboring centroids
dist <- nbdists(nb_b_1_2c, centroids)

# distance to each neighbor for the first 3 polygons
dist[1:3]

# calculate the inverse distance (1/distance) for all neighbors of every polygon
# lapply() takes our function (1/x) and applies it to every value in 'dist' list
#?lapply
dist_idw <- lapply(dist, function(x){1/x})

# inverse distance to each neighbor for the first 3 polygons
dist_idw[1:3]

# creating weights matrix as a 'list'
# 'glist' is used to assign the inverse distance weights to the correct neighbors
nbwt_idw_lst <- nb2listw(nb_b_1_2c, glist = dist_idw, style = "B", zero.policy = TRUE)

# elements in the list
# coding style
nbwt_idw_lst$style

# shows the neighbors, by row number, for the first polygon
nbwt_idw_lst$neighbours[1]

# shows the weights for the first polygon
nbwt_idw_lst$weights[1]

# spatial weights in a matrix
nbwt_idw_mat <- listw2mat(nbwt_idw_lst)

# weight matrix among the first ten polygons
nbwt_idw_mat[1:10, 1:10]

```

# 3. Spatial autocorrelation

To test for spatial autocorrelation, we'll use the spdep package to perform a global Moran's I test with `moran.test()` and a local Moran's I test with `localmoran()`.

## 3.1 Global

Recall, a global Moran's I test assesses if there is any clustering in the study area.

```{r}
# use our first order contiguity binary spatial weights file
# moran.test arguments <- x = numeric vector of cancer cases, listw = spatial weights in list format, 'alternative = greater' means we set a one-sided hypothesis that clustering is present, 'zero.policy = TRUE' accounts for islands
#?moran.test
gmi <- moran.test(x = lmi$CANCER, listw = nbwt_b_lst, alternative = "greater", zero.policy = TRUE)
gmi
```

The global Moran's I is 0.138, which is above 0 and indicating clustering. The p-value is less than 0.05 at 0.03. Thus, there is statistically significant clustering of lip cancer cases in Scotland. Although, the test statistic is admittedly small. What are the reasons why we might have a smaller test statistic?

## 3.2 Local

Because we know there is indeed global clustering, spatial regression rather than ordinary regression is necessary. However, running a local Moran's I will give us a idea of what we might expect to find from our spatial regression model. When we run localmoran(), instead of single overall parameters for the entire study areas, we will have test statistics, z-scores, and p-values for each county in the study.

```{r}
# the structure of the local moran's I function is the same as the global
lmi <- localmoran(x = scotlip$CANCER, listw = nbwt_b_lst, alternative = "greater", zero.policy = TRUE)

# NOTE: Row 6 is an island which is why it has all 0's and NaNs.
# Ii = Local Moran's I� statistic for each area
# E.Ii = Expectation Local Moran's I� statistic
# Var.Ii = Variance Local Moran's I� statistic
# Z.Ii = z-score
# Pr(z < E(Ii)) = p-value
head(lmi)

# changing column names for easier referencing
colnames(lmi) <- c('i', "e", "v", "z", "p")

# converting to data frame and combining with shapefile
scotlip_lmi <- cbind(scotlip, as.data.frame(lmi))

# creating a binary p-value variable for above and below p = 0.05
scotlip_lmi$p_bin <- ifelse(scotlip_lmi$p < 0.05, 1, 0)

# visualize local clustering using the local Moran's I statistic
ggplot() +
  geom_sf(data = scotlip_lmi, aes(fill = i)) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 1) +
  theme_void()

# visualize the uncertainty of the local clustering using the variance
ggplot() +
  geom_sf(data = scotlip_lmi, aes(fill = v)) +
  scale_fill_gradient(low = "white", high = "orange") +
  theme_void()

# visualize statistical significance of local clustering using the p-value
ggplot() +
  geom_sf(data = scotlip_lmi, aes(fill = factor(p_bin))) +
  scale_fill_manual(values = c("white","black"), labels = c("Non-sig", "Sig", "NA")) +
  theme_void()
```

`Ii`: Local Moran's I� statistic for each area,

`E.Ii`: Expectation Local Moran's I� statistic,

`Var.Ii`: Variance Local Moran's I� statistic,

`Z.Ii`: z-score,

`Pr(z > E(Ii))`, `Pr(z < E(Ii))` or `Pr(z != E(Ii))`: p-value for an alternative hypothesis `greater`, `less` or `two.sided`, respectively."

```         
lmoran <- localmoran(map$vble, nbw, alternative = "greater")
head(lmoran)

tm_shape(map) + tm_fill(col = "quadrant", title = "",
breaks = c(1, 2, 3, 4, 5, 6),
palette =  c("red", "blue", "lightpink", "skyblue2", "white"),
labels = c("High-High", "Low-Low", "High-Low",
           "Low-High", "Non-significant")) +
tm_legend(text.size = 1)  + tm_borders(alpha = 0.5) +
tm_layout(frame = FALSE,  title = "Clusters")  +
tm_layout(legend.outside = TRUE)
```

-   <https://www.paulamoraga.com/book-spatial/spatial-autocorrelation.html>

# 4. Spatial regression

## 4.1 Geographically weighted regression (GWR)

## 4.2 Integrated nested laplace approximation (INLA)

INLA

What data? Scottish lip cancer?

```         
install.packages("INLA",
repos = "https://inla.r-inla-download.org/R/stable", dep = TRUE)
library(INLA)
```

-   <https://www.paulamoraga.com/book-spatial/disease-risk-modeling.html>

-   <https://r-spatial.org/book/16-SpatialRegression.html>

# 5. Your turn

## 5.1 Regression

## 5.2 Disease map

# End

```{r}
library(CARBayesdata)
data()

data(lipdata)

library(sf)
library(spatstat.data)
library(gstat)

library(spatstat)

lmi <- st_read("https://geodacenter.github.io/data-and-lab/data/lmi.zip")


data(chicago)
#https://search.r-project.org/CRAN/refmans/spatstat.data/html/chicago.html

data(chicago)
  if(require(spatstat.linnet)) {
plot(chicago)
#plot(as.linnet(chicago), main="Chicago Street Crimes",col="green")
plot(as.ppp(chicago), add=TRUE, col="red", chars=c(16,2,22,17,24,15,6))
  }

library(maptools)

chi_pp <- as.SpatialPointsDataFrame.ppp(as.ppp(chicago))

chi_pp2 <- st_as_sf(as.ppp(chicago))

chi_pp3 <- st_as_sf(chicago)

class(chicago)

ggplot(chi_pp3 %>% filter(label != "segment")) + 
  geom_sf()

view(as.ppp(chicago))

st_coordinates(chi_pp)

view((chicago[["data"]]))

data(nbfires)

#https://hughst.github.io/week-1/
#https://malariaatlas.org/


```

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

1.  

2.  Importing and downloading data

    1.  <https://mgimond.github.io/Spatial/reading-and-writing-spatial-data-in-r.html>

    2.  <https://www.paulamoraga.com/book-spatial/the-sf-package-for-spatial-vector-data.html>

        1.  CSV

        2.  Shapefiles

        3.  Census

3.  Writing data

    1.  CSV

    2.  Shapefiles

4.  Basic data processing

    1.  Sf and Raster package features

    2.  Create new shapefile

        1.  Filter?

    3.  Coordinate systems

        1.  Setting, checking, comparing

    4.  ...

5.  Make a map

    1.  tmap

    2.  ggplot

        1.  Inset?

        2.  Arrow/Scale bar

        3.  Theme

        4.  Color scales

    3.  Leaflet

    4.  <https://mgimond.github.io/Spatial/mapping-rates-in-r.html>

    5.  <https://mgimond.github.io/Spatial/mapping-data-in-r.html>

6.  Descriptive statistics

    1.  Aspatial

    2.  Spatial

        1.  Intersection to count overlapping points

7.  Point pattern analysis

    1.  <https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html>

        1.  Kernel density

        2.  G-function

8.  Spatial autocorrelation

    1.  <https://mgimond.github.io/Spatial/spatial-autocorrelation-in-r.html>

        1.  Local Moran's I

            1.  Map

9.  Creating a spatial weights matrix

10. INLA

11. Final map

# References

Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). "Welcome to the tidyverse." *Journal of Open Source Software*, **4**(43), 1686. [doi:10.21105/joss.01686](https://doi.org/10.21105/joss.01686).

Edzer Pebesma, 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal [10:1, 439-446.](https://journal.r-project.org/archive/2018/RJ-2018-009/index.html)

Cressie, N (1991), *Statistics for spatial data*. New York: Wiley, pp. 386\--389; Cressie, N, Chan NH (1989) Spatial modelling of regional variables. *Journal of the American Statistical Association*, 84, 393\--401; Cressie, N, Read, TRC (1985) Do sudden infant deaths come in clusters? *Statistics and Decisions* Supplement Issue 2, 333\--349; <http://sal.agecon.uiuc.edu/datasets/sids.zip.>

<https://www.paulamoraga.com/book-spatial/disease-risk-modeling.html>

<https://r-spatial.org/book/16-SpatialRegression.html>

<https://www.paulamoraga.com/book-spatial/spatial-autocorrelation.html>

<https://www.paulamoraga.com/book-spatial/spatial-neighborhood-matrices.html>

<https://www.paulamoraga.com/book-spatial/the-k-function.html>

<https://arc2r.github.io/book/Density.html#kernel-density>

<https://r-spatial.org/book/11-PointPattern.html>

<https://r-spatial.org/book/11-PointPattern.html#marked-point-patterns-points-on-linear-networks>

<https://www.paulamoraga.com/book-geospatial/sec-spatialdataandCRS.html>

<https://arc2r.github.io/book/Static_Maps.html>

<https://ggplot2-book.org/>

# 

# 
