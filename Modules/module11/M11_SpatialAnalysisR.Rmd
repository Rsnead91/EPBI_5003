---
title: "M11 - Spatial data processing, analysis, and mapping in R"
instructor: ""
course: "EPBI 5003"
---

# Module outline

1.  R Packages for spatial data management, analysis, and visualization
2.  The `sf` package
3.  Importing and downloading data
4.  Writing data
5.  Spatial data processing
6.  Creating a map in R
7.  Point pattern analysis
8.  Creating a spatial weights matrix
9.  Spatial autocorrelation
10. Spatial Regression

# **1.** R Packages for spatial data management, analysis, and visualization

1.  **sf**: Offers simple features for spatial vector data, providing a modern and unified way to work with spatial data. Excellent for typical data processing. **(processing/management)**

2.  **leaflet**: Allows the creation of interactive maps through an interface with the Leaflet JavaScript library. **(visualization)**

3.  **tidycensus**: Enables easy access to US Census Bureau data, allowing users to download and work with US census data. **(processing/management)**

4.  **tigris**: Works in conjunction with the US Census Bureau's TIGER/Line data, providing functions to work with spatial data within R. Easy to use functions for downloading geographic boundaries. **(processing/management)**

5.  **sp**: Provides classes and methods for spatial data in R, offering infrastructure for handling spatial data objects. **THIS PROGRAM WAS REPLACED BY SF.** **(processing/management)**

6.  **raster**: Focuses on handling raster data (gridded data), offering functionalities for reading, writing, and manipulating such data. **(processing/management)**

7.  **maptools**: Offers tools for reading and handling spatial data in various formats and performing various manipulations. **(visualization)**

8.  **rgdal**: Serves as an interface to the Geospatial Data Abstraction Library (GDAL) for reading and writing raster and vector data formats. **(processing/management)**

9.  **rgeos**: Provides bindings to the GEOS library for geometric operations on geometries, useful for spatial data handling. **(processing/management)**

10. **osmdata**: Facilitates access to OpenStreetMap data, allowing users to download and work with OpenStreetMap data. **(processing/management)**

11. **ggmap**: Integrates mapping functionalities from Google Maps into ggplot2, allowing the creation of maps using Google Maps. **(visualization)**

12. **spData**: Offers a collection of spatial datasets to facilitate learning and teaching spatial data analysis. **(processing/management)**

13. **tidygeocoder**: Provides access to geocoding services, allowing the conversion of addresses into geographic coordinates. **(analysis)**

14. **tmap**: Enables the creation of thematic maps using layers, providing an interface to create static or interactive maps. **(visualization)**

15. **rastervis**: Offers visualization methods for raster data, facilitating the plotting and exploration of raster datasets. **(visualization)**

16. **gstat**: Focuses on geostatistical modeling, offering functionalities for spatial prediction and analysis. **(analysis)**

17. **spatstat**: Provides tools for spatial point pattern analysis, enabling statistical analysis of point pattern datasets. **(analysis)**

18. **geosphere**: Offers functions for calculating distances and areas in geographic coordinates, focusing on earth sciences. **(analysis)**

19. **ggspatial**: Extends ggplot2 with spatial layers and annotations, including north arrows and scale bars. **(visualization)**

20. **stars**: Provides a data structure and functions for raster data analysis, supporting large datasets in a multi-dimensional array format. **(processing/management/analysis)**

21. **terra**: Focuses on raster data handling and analysis, offering functionality for big data in earth observation analysis and spatial statistics. **(processing/management/analysis)**

# 2. The [Tidyverse](https://www.tidyverse.org/)

The **tidyverse** is a collection of R packages specifically created to streamline and enhances\\ the data science workflow by offering a consistent and cohesive set of tools. It's built around the concept of tidy data, focusing on data manipulation, visualization, and modeling. Just by loading the **tidyverse** library, you also load all of the packages within the **tidyverse**.

NOTE: After installing a package, you can comment out that line of code.

```{r load tidyverse, echo=FALSE}
#install.packages("tidyverse")
library(tidyverse)
```

Key packages in the Tidyverse include:

1.  [**`dplyr`**](https://dplyr.tidyverse.org/)**:** Offers a collection of functions for data manipulation, such as filtering, arranging, selecting, mutating, and summarizing data.

2.  [**`ggplot2`**](https://ggplot2.tidyverse.org/)**:** Facilitates the creation of sophisticated and customizable data visualizations using the grammar of graphics.

3.  [**`tidyr`**](https://tidyr.tidyverse.org/)**:** Primarily used for data tidying tasks, especially for reshaping data and handling missing values.

4.  [**`readr`**](https://readr.tidyverse.org/)**:** A set of functions for reading various types of data files into R.

5.  [**`readxl`**](https://readxl.tidyverse.org/)**:** Provides functions to read data from Excel files into R.

6.  [**`stringr`**](https://stringr.tidyverse.org/)**:** Offers a set of functions designed for efficient and convenient string manipulation in R, such as pattern matching, substring extraction, and string modification.

7.  [**`purrr`**](https://purrr.tidyverse.org/)**:** Provides tools for working with functions and vectors iteratively, enabling smoother functional programming workflows.

We will use the **tidyverse** to work with and manipulate data for our spatial analysis tasks.

# 3. The [sf](https://r-spatial.github.io/sf/) package

Most spatial analyses in public health work with vector data (i.e., polygons, points, lines) and the sf package can handle all of your spatial data processing needs.

## 3.1 `sf` objects

Rather than data frames, the **sf** package works with **sf** objects. These objects expand on data frames to incorporate spatial features, including *geometry*. An **sf** object can be one of three classes: sf (simple feature), sfc (simple feature geometry list-column), or sfg (simple feature geometry). Notably, **sf** objects are compatible with tidyverse functions.

The sf object presents as a normal data frame, but now contains a geometry column. Note, this is not an additional variable, but geometry data attached to the data frame. The geometry column of an **sf** object contains the following metadata: CRS, bounding box for the object, precision, and \# of empty geometries. See below for an example.

The following code loads a data set included in the **sf** package. Using the `head()` function, we see the data is of sf or "Simple Feature" class, the geometry is a MULTIPOLYGON meaning polygon rather than point data, has the bounding box details, and the CRS for the object (NAD27).

```{r Read NC data, echo=TRUE, paged.print=FALSE}
# load libraries
#install.packages("sf")
library(sf)

# import (read) north carolina sids data from the sf package
nc_sids <- st_read(system.file("shape/nc.shp", package = "sf"))

head(nc_sids)
```

## 3.2 functions

**Importing and Exporting Spatial Data**: `sf` can import and export spatial data in various formats like ESRI Shapefiles and GeoJSON using the `st_read()` and `st_write()` functions. Additionally, `st_as_sf()` converts non-sf objects, such as data frames, data tables, and tibbles, to sf objects.

**Coordinate Reference Systems (CRS)**: The `st_crs()` and `st_transform()` are used to check, set, and project CRS.

**Spatial Joins and Aggregations**: `sf` functions for spatial joins, `st_join()`, and spatial aggregation, `st_aggregate()`, to combine data based on spatial relationships or summarize data within defined regions.

**Manipulation of Spatial Data**: `sf` can subsetting, merging, transforming, and summarizing geometries. Common functions include `st_union()`, `st_difference()`, `st_buffer()`, `st_transform()`, etc. `st_coordinates()` pulls the coordinates of geometric objects.

**Overlay**: `sf` uses `st_intersects()`, `st_contains()`, `st_within()`, etc., to determine and output spatially related geometries.

**Visualization**: `sf` integrates with `ggplot2` through the `geom_sf()` layer.

**Geometric Operations**: `sf` performs geometric operations on spatial objects, including `st_area()`, `st_length()`, and `st_centroid()`.

**Attributes and Data Manipulation**: `sf` enables the management of both spatial and non-spatial data associated with spatial objects. Functions like `st_drop_geometry()`, `mutate()` from `dplyr`, and `select()` help manage attribute data.

Here is a [cheat sheet](https://github.com/rstudio/cheatsheets/blob/main/sf.pdf) to quickly refer back to for the name of sf functions and their brief descriptions.

![](sf_Page_1.png)

![](sf_Page_2.png)

# 4. Importing and exporting data

Like anything in R, there are many ways to perform the same task. When it comes to importing data, there are three general categories based on the type of data you are importing and where it is coming from:

## 4.1 Non-spatial data local on your computer

Fairly often, you will be given a CSV or excel file with spatial data (e.g., coordinates of cancer cases) or attributes (e.g., county-level rates of infant mortality) that need to be merged with spatial data. To import these "flat-files", we can use functions from the `readr` and `readxl` packages of the `tidyverse`:

-   `read_csv()`: comma delimited files

-   `read_tsv()`: tab delimited files

-   `read_delim()`: files with any delimiter

-   `read_excel()`: .xls or .xlsx files

To export and save data from R to your local computer as these flat-file formats, you use same functions but instead of the "read\_" prefix, use "write\_".

Example:

```{r, echo=TRUE}
# ignore the step
# creating a csv on your local computer to practice importing
nc_sids_nogeo <- nc_sids %>% st_drop_geometry()
write_csv(nc_sids, "nc_sids_nogeo.csv")
rm(nc_sids_nogeo)

# importing a csv file
nc_sids_nogeo <- read_csv("nc_sids_nogeo.csv")
#?read_delim
#?read_excel

# exporting a csv file
write_csv(nc_sids_nogeo, "export_csv.csv")
#?write_csv
```

For R files, .Rdata or .rds, you can use the "base" functions `load()` and `readRDS()`, respectively. Conversely, to export an .Rdata or .rds file, you would use the save() and writeRDS().

The primary difference between \`.Rdata\` and \`.rds\` files lies in how they store data in R.

-   .Rdata: These files can contain multiple objects (data frames, lists, variables) saved within a single file. They retain the internal structure of the R workspace, preserving all objects present when saving. When loading .Rdata files, all objects stored within the file are restored into the R environment.

-   .rds: These files are used to save a single R object. They store a single object in a serialized format, which means that only the object itself is saved without retaining any other workspace information. .rds files are more flexible and can be easily shared across different R versions or even with other programming languages. When loading .rds files, only the saved object is restored, allowing greater control over which specific objects to import into the R environment.

```{r, echo=TRUE}
# NOTE: .Rdata does not need to be assigned to an object, but .rds does.

save(nc_sids_nogeo, file = "nc_sids_r.Rdata")
#?save
load("nc_sids_r.Rdata")
#?load

write_rds(nc_sids_nogeo, "nc_sids_r2.rds")
#?write_rds
nc_sids_rds <- read_rds("nc_sids_r2.rds")
#?readRDS
```

## 4.2 Spatial data local on your computer

To import spatial files (i.e., ESRI shapefiles, GeoJSONs, KMLs, etc.) of vector data, you can use `st_read()` of the **sf** package. Another option is the `readOGR()` function of the **rgdal** package. Alternatively, spatial files can be created and exported with `st_write()` and `writeOGR()`.

```{r, echo=TRUE}
# import (read) north carolina sids data from the sf package
nc_sids <- st_read(system.file("shape/nc.shp", package = "sf"))
#?st_read

# NOTE: the code above uses system.file() because it is pulling data hosted by the sf package. If the the "nc.shp" file was on your computer in a subfolder called "shape" the code would be:
#nc_sids <- st_read("shape/nc.shp")

st_write(nc_sids, "nc_sids_shp.shp")
#?st_write
```

If you're working with raster data, you would perform similar procedures with the `raster()` and `writeRaster()` functions of the **raster** package or the `rast()` and `writeRaster()`functions of the **terra** package. However, the **terra** package is becoming more popular.

## 4.3 Importing data from external sources

In R, there are various methods to import data from R packages, fetch data from online sources like APIs, or retrieve information through URLs:

1.  **R Packages:** Many R packages come with built-in datasets that can be accessed directly. These datasets are often used for practice, testing, or as examples in documentation. You can load these datasets using functions like `data()`, `library()`, or directly calling the datasets by name.

```{r, echo=TRUE}
# check for data in loaded libraries
data()

# reference the name of the data set to load it
data(mtcars)
head(mtcars)
```

2.  **URLs:** R enables you to fetch data directly from URLs using functions like `download.file()` or higher-level functions like `read_delim()` to read data from URLs pointing to CSV files, text files, or other formats.

```{r, echo=TRUE}
# loading data directly from github
malaria <- read_csv("https://raw.githubusercontent.com/Rsnead91/EPBI_5003/main/malaria.csv")
```

3.  **APIs:** APIs (Application Programming Interfaces) are used to retrieve data from web services or online databases. R provides packages like `httr`, `jsonlite`, and `httr` that facilitate interaction with APIs. You can use functions like `GET()`, `POST()`, and `fromJSON()` to send requests to APIs and parse the response data (usually in JSON or XML format) into R objects.

4.  **Tidycensus for Census Data:** `tidycensus` is an R package specifically designed for accessing US Census Bureau data. It allows users to retrieve census data for various geographic levels (e.g., states, counties, tracts) and variables, providing a straightforward interface to access demographic, social, and economic data from the U.S. Census Bureau's API.

5.  **Webscraping:** There are numerous R packages designed for retrieving data from specific sources. For example, `rvest` and `xml2` are used for web scraping HTML/XML content, `ROAuth` for accessing Twitter APIs, and `Quandl` for fetching financial and economic datasets.

# 5. Spatial data processing and basic analysis

## 5.1 Review coordinate systems

Using the `nc_sids` sf object, let's check the coordinate system.

```{r, echo=TRUE}
# checking the assigned crs for nc_sids
st_crs(nc_sids)
```

From the output, we can see the CRS is set to `NAD27`, which is referred to by the EPSG code `4267`. Additionally, the `st_crs()` function returns outputs CRS information like the datum and prime meridian. This [website](https://guides.library.duke.edu/r-geospatial/CRS) has a list of common CRS EPSG codes.

Let's change the `nc_sids` CRS from NAD27 to USA Contiguous Albers Equal Area Conic, which has an EPSG code of `5070`.

```{r, echo=TRUE}
# changing the crs of the original nc_sids sf object
nc_sids_5070 <- st_transform(nc_sids, crs = 5070)

# check the new object is set to the new crs
st_crs(nc_sids_5070)
```

CRS was successfully changed. `nc_sids_5070` now has a geographic coordinate system of `NAD83` and a projection of `CONUS Albers`. CONUS means Contiguous US.

Now let's visually compare the original and transformed data using `ggplot()` with the `geom_sf()`. When only the data is referenced, and no aesthetics are assigned, ggplot returns the sf object's geometry in a longitude/latitude grid.

```{r, echo=TRUE}
# plot the nc_sids sf object
ggplot(nc_sids) +
  geom_sf()

# plot the nc_sids_5070 sf object
ggplot(nc_sids_5070) +
  geom_sf()
```

From the ggplot output, our two sf objects appear to have the correct CRS applied.

## 5.2 Manipulating spatial data

On every project you're going to have to modify your data in some way to get it in the format you need. For example, removing rows with missing data, subsetting your shapefiles to a specific study area, renaming columns, etc.. Fortunately, the **sf** package integrates with the **tidyverse** so all the data management functions available from those packages can be applied to sf objects.

Take a look at the `nc_sids` data. This [data set](https://jakubnowosad.com/spData/reference/nc.sids.html) contains the number of births, non-white births, and sudden infant deaths at the county-level in North Carolina from 1974-1978 and 1979-1984.

```{r, echo=TRUE}
print(nc_sids)
```

Suppose we want to remove all counties that had 0 SIDs from 1979-1984. We could filter the data to only keep rows with a value for SID79 \> 0. The `filter()` function is from the **dplyr** package of the **tidyverse**.

```{r, echo=TRUE}
# how many counties have 0 SIDS from 1979-1984
# table() outputs the frequencies of each value
# to refer to a specific variable in a data set, we can use '$' followed by the variable name
table(nc_sids$SID79)

# how many rows (counties) are in NC
length(nc_sids$SID79)
```

9 counties have 0 SIDS from 1979-1984. The `length()` function returns the number of rows in the data. Each row corresponds to a county in North Carolina. Therefore, after filtering our data, we would expect to 91 (100-9) remaining records.

```{r, echo=TRUE}
# remove counties that have 0 SIDS from 1979-1984
nc_sids_no0 <- filter(nc_sids, SID79 > 0)

# how many rows (counties) are there
length(nc_sids_no0$SID79)
```

Our new data set has 91 rows as expected. Let's visualize the new data to see how removing these counties impacts our map. Additionally, I will tell ggplot to add a color gradient to the count of SIDS from 1979-1984.

```{r, echo=TRUE}
# review the the nc shapefile before and after removing rows for counties without SIDS
ggplot(nc_sids) + 
  geom_sf()

ggplot(nc_sids_no0) + 
  geom_sf()

# adding a color gradient based on the values in SID79
# without specifying a color scheme R uses a default
# note: 'fill' assigns color within the polygon and 'color' assigns color to its borders
ggplot(nc_sids) + 
  geom_sf(aes(color = SID79, fill = SID79))

ggplot(nc_sids_no0) + 
  geom_sf(aes(color = SID79, fill = SID79))
```

## 5.3 Basic spatial data analysis

Let's perform some common basic spatial data processing and analysis. PFAS has historically been dumped in landfills around Pennsylvania (PA). This discharge can effect the soil and nearby drinking water, and have an adverse effect on public health.

To calculate how many landfills are in each county in PA, we could perform an **intersect** between landfill point data and PA county polygons.

The [tigris](https://cran.r-project.org/web/packages/tigris/tigris.pdf) package allows us to easily import shapefiles for common geographic boundaries in the US. We will use this package to access PA county data. Additionally, we will import a shapefile of landfill locations created for this class from data hosted on <https://www.pasda.psu.edu/>.

```{r}
#install.packages("tigris")
library(tigris)

# creating a shapefile of PA county cartographic boundaries ('cb = TRUE')
pa_counties <- counties(state = "PA", cb = TRUE)

# importing a zipped folder of all four shapefile for all the residential and municipal landfills in PA
# data comes from https://www.pasda.psu.edu/

# to import and unzip a folder of multiple files, you follow the following steps
# create two temporary files
t1 <- tempfile()
t2 <- tempfile()

# use download.file() for zipped folders and assign to the first temporary file
pa_landfills <- download.file("https://raw.githubusercontent.com/Rsnead91/EPBI_5003/main/pa_landfills.zip",t1)

# unzip the first temporary file and export to the second temporary file which is used as a directory to host multiple files since we need four for a single shapefile
unzip(zipfile = t1, exdir = t2)

# read-in the entire temporary directory to create the new shapefile of PA landfills
pa_landfills <- st_read(t2)

```

Once the data has been imported, we can create a simple map to visualize the polygon and point data to ensure everything looks right. To add multiple spatial layers to a `ggplot()`, we just include a `geom_sf()` for each layer. However, because we are using multiple layers, the data needs to specified in `geom_sf()`.

```{r}
# plot of PA counties and landfill locations in the state
ggplot() +
  geom_sf(data = pa_counties) +
  geom_sf(data = pa_landfills)
```

To count the number of points in each county, we can use st_intersects(). However, this function provides a "list" for each county of the point IDs that intersect it. We just need to pull out the "length" or number of IDs that intersected with each county. See below.

```{r}
# original intersect list output
landfill_count <- st_intersects(pa_counties, pa_landfills)

# take a look around the list to get familiar with this data type, if you want
#view(landfill_count)

# instead, we'll pull out the lengths of each list item and then add it as a variable to a new data set with county names
# creating a new sf object with only the county name
county_names <- dplyr::select(pa_counties, NAME)

# creating a data frame that is a single column with the counts of landfills for each county
landfill_count_df <- data.frame(count = lengths(st_intersects(pa_counties, pa_landfills)))

# since I know the order of the county names and landfill counts are the same, I can simply combine these two objects together using cbind()
county_landfill_counts <- cbind(county_names, landfill_count_df)

print(county_landfill_counts)
```

If we wanted to subset our landfill point data to a specific area (e.g., Erie County) and we don't have column values to filter on (e.g., county name), we could use `st_intersection()`, different from `st_intersects()`, which creates a new sf object based on overlapping features.

```{r}
# first reduce pa_counties to only erie county
erie <- filter(pa_counties, NAME == "Erie")

# run st_intersection() with the area we want out points to subset to first and the features we want to subset second
erie_landfills <- st_intersection(erie, pa_landfills)

# check if we were successful
ggplot() +
  geom_sf(data = erie) +
  geom_sf(data = erie_landfills)
```

Let's import the census tracts for Erie County, PA and calculate the distance from each tract centroid to the nearest landfill. We can use the **tigris** package to download the census tracts. From the **sf** package, we can create centroids with `st_centroid()` and use `st_nearest_feature()` and `st_distance()` to find the distance to the closest landfill for each centroid.

First, we'll pull the census tracts for Erie County.

```{r}
# get pa census tracts for erie county (fips = 049) using 2010 decennial census boundaries
erie_tracts <- tracts(state = "PA", county = 049, cb = TRUE, year = 2010)

# double check output
ggplot() +
  geom_sf(data = erie_tracts) +
  geom_sf(data = erie_landfills)
```

Next, we'll create the centroids for each census tract.

```{r}
# calculate census tract centroids
erie_centroids <- st_centroid(erie_tracts)

# double check output
ggplot() +
  geom_sf(data = erie_tracts) +
  geom_sf(data = erie_landfills, color = "black") +
  geom_sf(data = erie_centroids, color = "red")
```

Easy as pie. Once we have the centroids, we can calculate distances for each census tract centroid to the nearest landfill. This step is not quite as straightforward. First, for each centroid, we need to identify the landfill that is the closest.

```{r}
# identify the closest landfill to each centroid
# st_nearest_feature returns the row number of the nearest landfill for each tract centroid
landfill_near <- st_nearest_feature(erie_centroids, erie_landfills)

# the output is just a vector of row numbers
# note there are 71 values and 71 census tracts in erie, so we were successful in calculating the nearest landfill for each tract
print(landfill_near)
```

Then, we calculate distance to the nearest landfill that was identified by subsetting the landfill data.

```{r}
# calculate the distance
# distance is returned in meters because these are the units used in the crs
dist <- st_distance(erie_centroids, erie_landfills[landfill_near,], by_element = TRUE)

# as.numeric converts the distance values to numeric format
dist_df <- data.frame(landfill_near = as.numeric(dist))
```

Wonderful! The distances between each centroid and the nearest landfill have been calculated. However, if we wanted to make a thematic map of the tract polygons based on the distances we just calculated, we wouldn't be able to. The `erie_tracts` object does not have those distance values. Thus, in order to create a thematic map of the distances by census tract, we need to combine our data.

```{r}
# combine with erie_tracts
tract_fill_dist <- cbind(erie_tracts, dist_df)

# printing the combined data subset to only the tract and landfill_near variables
print(tract_fill_dist[, c("TRACT","landfill_near")])

# visualize census tracts with high and low distances from their centroid to a landfill 
ggplot() +
  geom_sf(data = tract_fill_dist, aes(fill = landfill_near))
```

# 6. Creating a map in R

When it comes to making maps in R, there are two main types: static and interactive. Static maps are the figures we have already been making in this course, whereas interactive maps are what you might find on dashboards or on the internet. Interactive maps allow you to do things like click on, or hover over, areas to explore additional information for that specific place. But first, let's go through the features of static maps.

## 6.1 Static map

Like we have been already, we will work in [ggplot](https://ggplot2.tidyverse.org/). There are other packages used for mapping like **tmap**, **mapview**, **raster**, and **base R**, but ggplot is the most widely used visualization tool in R and easy to work with. We will continue with ggplot. There is so much that you could cover with ggplot. We will only go over the necessary information for this course. [Here](https://ggplot2-book.org/) is a comprehensive resource if you want to learn more about ggplot.

**Basics:** ggplot has a simple grammar, which builds from the following structure/order of elements:

1.  Data.

2.  Geometry. These are the shapes of the visual (e.g., lines, bars) and are specified from `geom_*()` functions (e.g., `geom_point()`).

3.  Aesthetics. The aesthetics of the geometry are things like the color, size, and shape, used to visualize information through the plot. These are referenced within `geom_*() functions` in the `aes()`.

4.  Formatting. The remaining modifiable elements of a figure, including things like themes (e.g., font, ticks, plot size), titles, labels, and legends.

When we "add" elements to our plot, we use a "+" to piece them together. You've probably noticed this in prior plots. NOTE: This is different from piping functions together using `%>%` or `|>` in open code. These pipes will not work to stitch together ggplot elements.

Here is an example using all four elements:

```{r}

# data
ggplot(nc_sids) + 
  # geometry and aesthetics
  geom_sf(aes(color = SID79, fill = SID79)) +
  # optional elements: title
  labs(title = "sample map") +
  # optional elements: theme
  theme(
    # set all text to Arial font
    text = element_text(family = "Arial"),
    # set the background of the map to white
    panel.background = element_rect(fill = "white"),
    # remove all axis text (i.e., lat/long)
    axis.text = element_blank(),
    # remove all axis ticks
    axis.ticks = element_blank()
  )

```

### 6.1.1 Data

We must use a tibble, data frame, or sf object with ggplot. If you are using a single data source, the name of the data should be reference in the `ggplot()`. Otherwise, each source should be specified within their respective `geom_*()`. For example:

```         
ggplot(nc_sids) +
  geom_sf()
  
ggplot() +
  geom_sf(data = pa_counties) +
  geom_sf(data = pa_landfills)
```

### 6.1.2 Geometry

`geom_sf()` is the "geometry" for maps in ggplot and works for points, polygons, and lines.

### 6.1.3 Aesthetics

[Aesthetics](https://ggplot2.tidyverse.org/reference/aes.html) map to the defined variables, creating the visual properties. Depending on the geometry, different aesthetics are available. For `geom_sf()`, we must commonly work with:

-   **color:** lines, points, polygons (borders only)

-   **fill:** polygons (inside of polygon)

-   **shape:** points

-   **size:** lines, points

**Color:** This applies to both "color" and "fill" aesthetics. Assigning and changing color is the most challenging and versatile aspect of the aesthetics. Single colors can be specified by name or more specifically through hex codes.

```{r}
# 000000 and FFFFFF are the hexcodes for black and white, respectively.
ggplot() +
  geom_sf(data = pa_counties, color = "#000000", fill = "#FFFFFF") +
  geom_sf(data = pa_landfills, color = "red")
```

When it comes to color scales, there are a myriad of options.

Palletes

Manual

The default to set color scales is scale\_\*\_continuous()

Continuous scale: scale_color_continuous

viridis

RColorBrewer

-   continuous: scale\_\*\_distiller()

-   discrete: scale\_\*\_brewer()

Alternatively, you can manually set the scale with scale_manual...

set na values with na.value = NA or na.value = "red"

Also breaks...

### 6.1.4 Formatting

CRS

-   breaks

-   ggspatial

    -   arrow bar

-   <https://www.paulamoraga.com/book-geospatial/sec-spatialdataandCRS.html>

-   <https://arc2r.github.io/book/Static_Maps.html>

## 6.2 Interactive map

NC data?

scotlip data?

```         
# Define your basemap
basemap <- leaflet() %>% addTiles()
basemap

basemap %>% addPolygons(data=ETH_Adm_1)

basemap %>% addPolygons(data=ETH_Adm_1, color = "red", 
                        weight = 1, fillOpacity = 0.2, 
                        popup = ETH_Adm_1$NAME_1) %>%
  
            addCircleMarkers(data=ETH_malaria_data_SPDF,
                             color="green", radius = 2) %>%
      
      addLegend(pal = colorPal, 
                title = "Prevalence",
                values = ETH_malaria_data_SPDF$pf_pr)
                



library(leaflet)

pal <- colorNumeric("YlOrRd", domain = map$SID74)

leaflet(map) %>%
  addTiles() %>%
  addPolygons(
    color = "white", fillColor = ~ pal(SID74),
    fillOpacity = 1
  ) %>%
  addLegend(pal = pal, values = ~SID74, opacity = 1)
  
  
```

# 7. Point pattern analysis

spatstat package?

-   st_sample

    -   random

    -   clustered

    -   kappa \<- 30 / [st_area](https://r-spatial.github.io/sf/reference/geos_measures.html)(w2) \# intensity
        th \<- [st_sample](https://r-spatial.github.io/sf/reference/st_sample.html)(w2, kappa = kappa, mu = 3, scale = 0.05,
        type = "Thomas")
        [nrow](https://rdrr.io/r/base/nrow.html)(th)

-   <https://r-spatial.org/book/11-PointPattern.html#marked-point-patterns-points-on-linear-networks>

Kernel density estimation

-   simulated data

-   <https://arc2r.github.io/book/Density.html#kernel-density>

-   <https://r-spatial.org/book/11-PointPattern.html>

    -   den1 \<- [density](https://rdrr.io/r/stats/density.html)(pp1, sigma = bw.diggle)

G or K-function

-   malaria data

-   K: <https://www.paulamoraga.com/book-spatial/the-k-function.html>

```         
library(spatstat)
X <- rpoispp(lambda = 100)
plot(X)
axis(1)
axis(2)

K <- Kest(X)
plot(K)

E <- envelope(X, Kest, nsim = 99)
plot(E)
```

# 8. Creating a spatial weights matrix

<https://www.paulamoraga.com/book-spatial/spatial-neighborhood-matrices.html>

## 8.1 Contiguity

-   Types of objects: nb, list, mat..

```         
library(spData)
library(sf)
library(spdep)
library(ggplot2)

map <- st_read(system.file("shapes/columbus.shp",
               package = "spData"), quiet = TRUE)

nb <- spdep::poly2nb(map, queen = TRUE)
head(nb)

plot(st_geometry(map), border = "lightgray")
plot.nb(nb, st_geometry(map), add = TRUE)

id <- 20 # area id
map$neighbors <- "other"
map$neighbors[id] <- "area"
map$neighbors[nb[[id]]] <- "neighbors"
ggplot(map) + geom_sf(aes(fill = neighbors)) + theme_bw() +
  scale_fill_manual(values = c("gray30", "gray", "white"))
```

## 8.2 Distance-based

-   knn - num

```         
# Neighbors based on 3 nearest neighbors
coo <- st_centroid(map)
nb <- knn2nb(knearneigh(coo, k = 3)) # k number nearest neighbors
plot(st_geometry(map), border = "lightgray")
plot.nb(nb, st_geometry(map), add = TRUE)
```

-   knn - distance

```         
# Neighbors based on distance
nb <- dnearneigh(x = st_centroid(map), d1 = 0, d2 = 0.4)
plot(st_geometry(map), border = "lightgray")
plot.nb(nb, st_geometry(map), add = TRUE)

coo <- st_centroid(map)
# k is the number nearest neighbors
nb1 <- knn2nb(knearneigh(coo, k = 1))

dist1 <- nbdists(nb1, coo)
summary(unlist(dist1))
```

IDW?

8.3 Neighbor weights matrix

```         

# binary
nb <- poly2nb(map, queen = TRUE)
nbw <- spdep::nb2listw(nb, style = "W")
nbw$weights[1:3]

m1 <- listw2mat(nbw)
lattice::levelplot(t(m1),
scales = list(y = list(at = c(10, 20, 30, 40),
                       labels = c(10 20, 30, 40))))

# idw
coo <- st_centroid(map)
nb <- poly2nb(map, queen = TRUE)
dists <- nbdists(nb, coo)
ids <- lapply(dists, function(x){1/x})

nbw <- nb2listw(nb, glist = ids, style = "B")
nbw$weights[1:3]

m2 <- listw2mat(nbw)
lattice::levelplot(t(m2),
scales = list(y = list(at = c(10, 20, 30, 40),
                       labels = c(10, 20, 30, 40))))
```

# 9. Spatial autocorrelation

Moran's I

-   Global

```         
# Neighbors
library(spdep)
nb <- poly2nb(map, queen = TRUE) # queen shares point or border
nbw <- nb2listw(nb, style = "W")

# Global Moran's I
gmoran <- moran.test(map$vble, nbw,
                     alternative = "greater")
gmoran

gmoran[["estimate"]][["Moran I statistic"]] # Moran's I
gmoran[["statistic"]] # z-score
gmoran[["p.value"]] # p-value
```

-   Local"

    -   `Ii`: Local Moran\'s I� statistic for each area,

    -   `E.Ii`: Expectation Local Moran\'s I� statistic,

    -   `Var.Ii`: Variance Local Moran\'s I� statistic,

    -   `Z.Ii`: z-score,

    -   `Pr(z > E(Ii))`, `Pr(z < E(Ii))` or `Pr(z != E(Ii))`: p-value for an alternative hypothesis `greater`, `less` or `two.sided`, respectively."

    ```         
    lmoran <- localmoran(map$vble, nbw, alternative = "greater")
    head(lmoran)

    tm_shape(map) + tm_fill(col = "quadrant", title = "",
    breaks = c(1, 2, 3, 4, 5, 6),
    palette =  c("red", "blue", "lightpink", "skyblue2", "white"),
    labels = c("High-High", "Low-Low", "High-Low",
               "Low-High", "Non-significant")) +
    tm_legend(text.size = 1)  + tm_borders(alpha = 0.5) +
    tm_layout(frame = FALSE,  title = "Clusters")  +
    tm_layout(legend.outside = TRUE)
    ```

-   <https://www.paulamoraga.com/book-spatial/spatial-autocorrelation.html>

# 10. Spatial Regression

INLA

What data? Scottish lip cancer?

```         
install.packages("INLA",
repos = "https://inla.r-inla-download.org/R/stable", dep = TRUE)
library(INLA)
```

-   <https://www.paulamoraga.com/book-spatial/disease-risk-modeling.html>

-   <https://r-spatial.org/book/16-SpatialRegression.html>

# 

# End

```{r}
library(CARBayesdata)
data()

data(lipdata)

library(sf)
library(spatstat.data)
library(gstat)

library(spatstat)

scotlip <- st_read("https://geodacenter.github.io/data-and-lab/data/scotlip.zip")


data(chicago)
#https://search.r-project.org/CRAN/refmans/spatstat.data/html/chicago.html

data(chicago)
  if(require(spatstat.linnet)) {
plot(chicago)
#plot(as.linnet(chicago), main="Chicago Street Crimes",col="green")
plot(as.ppp(chicago), add=TRUE, col="red", chars=c(16,2,22,17,24,15,6))
  }

library(maptools)

chi_pp <- as.SpatialPointsDataFrame.ppp(as.ppp(chicago))

chi_pp2 <- st_as_sf(as.ppp(chicago))

chi_pp3 <- st_as_sf(chicago)

class(chicago)

ggplot(chi_pp3 %>% filter(label != "segment")) + 
  geom_sf()

view(as.ppp(chicago))

st_coordinates(chi_pp)

view((chicago[["data"]]))

data(nbfires)

#https://hughst.github.io/week-1/
#https://malariaatlas.org/


```

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

1.  

2.  Importing and downloading data

    1.  <https://mgimond.github.io/Spatial/reading-and-writing-spatial-data-in-r.html>

    2.  <https://www.paulamoraga.com/book-spatial/the-sf-package-for-spatial-vector-data.html>

        1.  CSV

        2.  Shapefiles

        3.  Census

3.  Writing data

    1.  CSV

    2.  Shapefiles

4.  Basic data processing

    1.  Sf and Raster package features

    2.  Create new shapefile

        1.  Filter?

    3.  Coordinate systems

        1.  Setting, checking, comparing

    4.  ...

5.  Make a map

    1.  tmap

    2.  ggplot

        1.  Inset?

        2.  Arrow/Scale bar

        3.  Theme

        4.  Color scales

    3.  Leaflet

    4.  <https://mgimond.github.io/Spatial/mapping-rates-in-r.html>

    5.  <https://mgimond.github.io/Spatial/mapping-data-in-r.html>

6.  Descriptive statistics

    1.  Aspatial

    2.  Spatial

        1.  Intersection to count overlapping points

7.  Point pattern analysis

    1.  <https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html>

        1.  Kernel density

        2.  G-function

8.  Spatial autocorrelation

    1.  <https://mgimond.github.io/Spatial/spatial-autocorrelation-in-r.html>

        1.  Local Moran's I

            1.  Map

9.  Creating a spatial weights matrix

10. INLA

11. Final map

# References

Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). "Welcome to the tidyverse." *Journal of Open Source Software*, **4**(43), 1686. [doi:10.21105/joss.01686](https://doi.org/10.21105/joss.01686).

Edzer Pebesma, 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal [10:1, 439-446.](https://journal.r-project.org/archive/2018/RJ-2018-009/index.html)

Cressie, N (1991), *Statistics for spatial data*. New York: Wiley, pp. 386\--389; Cressie, N, Chan NH (1989) Spatial modelling of regional variables. *Journal of the American Statistical Association*, 84, 393\--401; Cressie, N, Read, TRC (1985) Do sudden infant deaths come in clusters? *Statistics and Decisions* Supplement Issue 2, 333\--349; <http://sal.agecon.uiuc.edu/datasets/sids.zip.>

<https://www.paulamoraga.com/book-spatial/disease-risk-modeling.html>

<https://r-spatial.org/book/16-SpatialRegression.html>

<https://www.paulamoraga.com/book-spatial/spatial-autocorrelation.html>

<https://www.paulamoraga.com/book-spatial/spatial-neighborhood-matrices.html>

<https://www.paulamoraga.com/book-spatial/the-k-function.html>

<https://arc2r.github.io/book/Density.html#kernel-density>

<https://r-spatial.org/book/11-PointPattern.html>

<https://r-spatial.org/book/11-PointPattern.html#marked-point-patterns-points-on-linear-networks>

<https://www.paulamoraga.com/book-geospatial/sec-spatialdataandCRS.html>

<https://arc2r.github.io/book/Static_Maps.html>

<https://ggplot2-book.org/>

# 

# 
